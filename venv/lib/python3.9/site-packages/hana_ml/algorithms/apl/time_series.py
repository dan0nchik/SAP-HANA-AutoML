#pylint: disable=too-many-lines
"""
This module contains the SAP HANA APL Time Series algorithm.

The following class is available:

    * :class:`AutoTimeSeries`
"""
import logging
import numpy as np
import pandas as pd
from hdbcli import dbapi
from hana_ml.dataframe import (
    DataFrame,
    quotename)
from hana_ml.ml_exceptions import Error, FitIncompleteError
from hana_ml.algorithms.apl.apl_base import (
    APLBase,
    APLArtifactTable,
    APLArtifactApplyOutTable)
from hana_ml.ml_base import (
    parse_one_dtype,
    ListOfStrings,
)

logger = logging.getLogger(__name__) #pylint: disable=invalid-name


class AutoTimeSeries(APLBase): #pylint: disable=too-many-instance-attributes
    """
    SAP HANA APL Time Series algorithm.

    Parameters
    ----------
    target: str
        The name of the column containing the time series data points.
    time_column_name: str
        The name of the column containing the time series time points.
        The time column is used as table key. It can be overridden by setting the 'key' parameter
        through the fit() method.
    last_training_time_point: str, optional
        The last time point used for model training.
        The training dataset will contain all data points up to this date.
        By default, this parameter will be set as the last time point until which the target is
        not null.
    horizon: int, optional
        The number of forecasts to be generated by the model upon apply.
        The time series model will be trained to optimize accuracy on the requested horizon only.
        The default value is 1.
    with_extra_predictable: bool, optional
        If set to true, all input variables will be used by the model to generate forecasts.
        If set to false, only the time and target columns will be used. All other variables
        will be ignored.
        This parameter is set to true by default.
    variable_storages: dict, optional
        Specifies the variable data types (string, integer, number).
        For example, {'VAR1': 'string', 'VAR2': 'number'}.
        See notes below for more details.
    variable_value_types: dict, optional
        Specifies the variable value types (continuous, nominal, ordinal).
        For example, {'VAR1': 'continuous', 'VAR2': 'nominal'}.
        See notes below for more details.
    variable_missing_strings: dict, optional
        Specifies the variable values that will be taken as missing.
        For example, {'VAR1': '???'} means anytime the variable value equals '???',
        it will be taken as missing.
    extra_applyout_settings: dict, optional
        Specifies the prediction outputs.
        See documentation on predict() method for more details.
    other_params: dict, optional
        Corresponds to the advanced settings.
        The dictionary contains {<parameter_name>: <parameter_value>}.
        The possible parameters are:
        - force_negative_forecast
        - force_positive_forecast
        - forecast_fallback_method
        - forecast_max_cyclics
        - forecast_max_lags
        - forecast_method
        - smoothing_cycle_length
        See *Common APL Aliases for Model Training* in the `SAP HANA APL Reference Guide
        <https://help.sap.com/viewer/p/apl>`_.
    other_train_apl_aliases: dict, optional
        Users can provide APL aliases as advanced settings to the model.
        Unlike 'other_params' described above, users are free to input any possible value.
        There is no control in python.

    Attributes
    ----------
    model_: hana_ml DataFrame
        The trained model content
    summary_: APLArtifactTable
        The reference to the "SUMMARY" table generated by the model training.
        This table contains the summary about the model training.
    indicators_: APLArtifactTable
        The reference to the "INDICATORS" table generated by the model training.
        This table contains the various metrics related to the model and its variables.
    fit_operation_logs_: APLArtifactTable
        The reference to the "OPERATION_LOG" table generated by the model training
    var_desc_: APLArtifactTable
        The reference to the "VARIABLE_DESCRIPTION" table that was built during the model training
    applyout_: hana_ml DataFrame
        The predictions generated the last time the model was applied
    predict_operation_logs_: APLArtifactTable
        The reference to the "OPERATION_LOG" table that is produced when making predictions.
    train_data_: hana_ml DataFrame
        The train dataset
    sort_data: bool
        If True, a temporary view is created on the dataset to sort data by time.
        However, users can provide directly a view with sorted dates.
        In this case, they must set *sort_data* to *False* to avoid creating a new view.
        The default value is True.
        WARNING: it is recommended to leave this parameter by default so the data is guaranteed
        to be read in sorted order. If the data is not sorted, the model will fail.

    Examples
    --------
    >>> from hana_ml.algorithms.apl.time_series import AutoTimeSeries
    >>> from hana_ml.dataframe import ConnectionContext, DataFrame

    Connecting to SAP HANA

    >>> CONN = ConnectionContext(HDB_HOST, HDB_PORT, HDB_USER, HDB_PASS)
    >>> # -- Creates Hana DataFrame
    >>> hana_df = DataFrame(CONN, 'select * from APL_SAMPLES.CASHFLOWS_FULL')

    Creating and fitting the model

    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(data=hana_df)

    Debriefing

    >>> model.get_model_components()
    {'Trend': 'Polynom( Date)',
     'Cycles': 'PeriodicExtrasPred_MondayMonthInd',
     'Fluctuations': 'AR(46)'}

    >>> model.get_performance_metrics()
    {'MAPE': [0.12853715702893018, 0.12789963348617622, 0.12969031859857874], ...}


    Generating forecasts using the **forecast()** method
    This method is used to generate forecasts using a signature similar to the one used in PAL.
    There are two variants of usage as described below:

    1) If the model does not use **extra-predictable** variables (no exogenous variable), users
    must simply specify the number of forecasts.

    >>> train_df = DataFrame(CONN,
                            'SELECT "Date" , "Cash" '
                            'from APL_SAMPLES.CASHFLOWS_FULL ORDER BY 1 LIMIT 100')
    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(train_df)
    >>> out = model.forecast(forecast_length=3)
    >>> out.collect().tail(5)
               Date                            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    98   2001-05-23  3057.812544999999772699132909775  4593.966530              NaN              NaN
    99   2001-05-25  3037.539714999999887176132440567  4307.893346              NaN              NaN
    100  2001-05-26                              None  4206.023158     -3609.599872     12021.646187
    101  2001-05-27                              None  4575.162651     -3392.283802     12542.609104
    102  2001-05-28                              None  4830.352462     -3239.507360     12900.212284


    2) If the model uses **extra-predictable** variables, users must provide the values of all
    extra-predictable variables for each time point of the forecast period.
    These values must be provided as a hana_ml dataframe with the same structure as the
    training dataset.

    >>> # Trains the dataset with extra-predictable variables
    >>> train_df = DataFrame(CONN,
    ...                     'SELECT * '
    ...                     'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                     'WHERE "Cash" is not null')
    >>> # Extra-predictable variables' values on the forecast period
    >>> forecast_df = DataFrame(CONN,
    ...                        'SELECT * '
    ...                        'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                        'WHERE "Cash" is null LIMIT 5')
    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(train_df)
    >>> out = model.forecast(data=forecast_df)
    >>> out.collect().tail(5)
              Date ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    251  2001-12-29   None  6864.371407      -224.079492     13952.822306
    252  2001-12-30   None  6889.515324      -211.264912     13990.295559
    253  2001-12-31   None  6914.766513      -187.180923     14016.713949
    254  2002-01-01   None  6940.124974              NaN              NaN
    255  2002-01-02   None  6965.590706              NaN              NaN


    Generating forecasts with the **predict()** method.
    The predict() method allows users to apply a fitted model on a dataset different from the
    training dataset.
    For example, users can train a dataset on the first quarter (January to March) and apply
    the model on a dataset of different period (March to May).

    >>> # Trains the model on the first quarter, from January to March
    >>> train_df = DataFrame(CONN,
    ...                     'SELECT "Date" , "Cash" '
    ...                     'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                     "where \"Date\" between '2001-01-01' and '2001-03-31'"
    ...                     " ORDER BY 1")
    >>> model.fit(train_df)
    >>> # Forecasts on a shifted period, from March to May
    >>> test_df = DataFrame(CONN,
    ...                    'SELECT "Date", "Cash" '
    ...                    'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                    "where \"Date\" between '2001-03-01' and '2001-05-31'"
    ...                    " ORDER BY 1")
    >>> out = model.predict(test_df)
    >>> out.collect().tail(5)
              Date                            ACTUAL     PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    60  2001-05-30  3837.196734000000105879735597214   4630.223083              NaN              NaN
    61  2001-05-31  2911.884261000000151398126928726   4635.265982              NaN              NaN
    62  2001-06-01                              None   4538.516542     -1087.461104     10164.494188
    63  2001-06-02                              None   4848.815364     -5090.167255     14787.797983
    64  2001-06-03                              None   4853.858263     -5138.553275     14846.269801

    Using the **fit_predict()** method
    This method enables the user to fit a model and generate forecasts on a single call, and thus
    get results faster. However, the model is created on the fly and deleted after use, so the user
    will not be able to save the resulting model.

    >>> model.fit_predict(hana_df)
    >>> out.collect().tail(5)
               Date            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    249  2001-12-27  5995.42329499999  6055.761105              NaN              NaN
    250  2001-12-28  7111.41669699999  6314.336098              NaN              NaN
    251  2002-01-03                           None  7033.880804      4529.462710      9538.298899
    252  2002-01-04                           None  6464.557223      3965.343397      8963.771049
    253  2002-01-07                           None  6469.141663      3961.414900      8976.868427

    Breaking down the time series into trend, cycles, fluctuations and residuals components.
    If the parameter *extra_applyout_settings* is set to **{'ExtraMode': True}**, anytime a forecast
    method is called, predict(), forecast() or fit_predict(), the output will contain time series
    components and their corresponding residuals. The prediction columns are suffixed by the
    horizon number. For instance, 'Cycles_RESIDUALS_3' means the residual of the cycle component in
    the third horizon.

    >>> model.fit(train_df)
    >>> model.set_params(extra_applyout_settings={'ExtraMode': True})
    >>> out = model.predict(hana_df)
    >>> out.collect().tail(5)
                   Date              ACTUAL        ...  Cycles_RESIDUALS_3  Fluctuations_RESIDUALS_3
    249  2001-12-27  5995.42329499392507553        ...               32.51                  4.48e-13
    250  2001-12-28  7111.41669699455205917        ...             -644.77                  1.14e-13
    251  2002-01-03                    None        ...                 NaN                       NaN
    252  2002-01-04                    None        ...                 NaN                       NaN
    253  2002-01-07                    None        ...                 NaN                       NaN


    Notes
    -----
    The input dataset, given as an hana_ml dataframe, must not be a temporary table because the
    API tries to create a view sorted by the time column.
    SAP HANA does not allow user to create a view on temporary table.
    However, even though it is **not recommended**, to avoid creating the view, user can force
    the parameter sort_data to False.

    When calling the fit_predict() method, the time series model is generated on the fly and
    not returned. If a model must be saved, please consider using the fit() method instead.

    When extra-predictable variables are involved, it is usual to have a single dataset used
    both for the model training and the forecasting. In this case, the dataset should contain two
    successive periods:
        The first one is used for the model training, ranging from the beginning to the last
        date where the target value **is not null**.

        The second one is used for the model training, ranging from the the first date where
        the target value **is null**.

    The content of the output of the get_performance_metrics() method may change depending of
    the version of SAP HANA APL used with this API. Please refer to the SAP HANA APL documentation
    to know which metrics will be provided.

    """
    # Define APL aliases for TimeSeries
    # See APL Help
    #     "Function Reference" > "Common APL Aliases for Model Training" > Time-Series Models
    # Override upper class static variable
    APL_ALIAS_KEYS = {
        # TS requires to use a sequential cutting strategy in all cases
        'force_negative_forecast': 'APL/ForceNegativeForecast',
        'force_positive_forecast': 'APL/ForcePositiveForecast',
        'forecast_fallback_method': 'APL/ForecastFallbackMethod',
        'forecast_max_cyclics': 'APL/ForecastMaxCyclics',
        'forecast_max_lags': 'APL/ForecastMaxLags',
        'forecast_method': 'APL/ForecastMethod',
        'smoothing_cycle_length': 'APL/SmoothingCycleLength',
        }

    def __init__(self,
                 conn_context=None,
                 time_column_name=None,
                 target=None,
                 horizon=1,
                 with_extra_predictable=True,
                 last_training_time_point=None,
                 variable_storages=None,
                 variable_value_types=None,
                 variable_missing_strings=None,
                 extra_applyout_settings=None,
                 train_data_=None,
                 sort_data=True,
                 ** other_params): #pylint: disable=too-many-arguments
        super(AutoTimeSeries, self).__init__(
            conn_context,
            variable_storages,
            variable_value_types,
            variable_missing_strings,
            extra_applyout_settings,
            ** other_params)
        self._model_type = 'timeseries'
        self.target = None
        self.with_extra_predictable = None
        self.set_params(time_column_name=time_column_name)
        self.set_params(target=target)
        self.set_params(horizon=horizon)
        self.set_params(
            with_extra_predictable=with_extra_predictable)
        self.set_params(
            last_training_time_point=last_training_time_point)
        self.set_params(
            extra_applyout_settings=extra_applyout_settings)
        self.model_ = None
        # dataframe used in the last fit, required for forecast() method
        self.train_data_ = train_data_
        self.set_params(sort_data=sort_data)

    def set_params(self, **parameters):
        """
        Sets attributes of the current model.

        Parameters
        ----------
        parameters: dict
            Contains attribute names and values in the form of keyword arguments
        """
        if 'target' in parameters:
            self.target = self._arg('target', parameters.pop('target'), str)
        if 'time_column_name' in parameters:
            self.time_column_name = self._arg('time_column_name',
                                              parameters.pop('time_column_name'),
                                              str)
        if 'horizon' in parameters:
            self.horizon = self._arg('horizon', parameters.pop('horizon'), int)
        if 'with_extra_predictable' in parameters:
            self.with_extra_predictable = self._arg('with_extra_predictable',
                                                    parameters.pop('with_extra_predictable'),
                                                    bool)
        if 'last_training_time_point' in parameters:
            self.last_training_time_point = self._arg(
                'last_training_time_point',
                parameters.pop('last_training_time_point'),
                str)
        if 'extra_applyout_settings' in parameters:
            param = self._arg(
                'extra_applyout_settings',
                parameters.pop('extra_applyout_settings'),
                dict)
            if param:
                # It is possible to reset the param to None
                if not set(param.keys()).issubset(['ExtraMode']):
                    msg = ("The extra_applyout_settings parameter must only contain "
                           "'ExtraMode' as key")
                    raise ValueError(msg)
                # Checks the value of param['ExtraMode']
                param_val = param['ExtraMode']
                if not isinstance(param_val, bool):
                    msg = ("'ExtraMode' must be a boolean")
                    raise TypeError(msg)
            self.extra_applyout_settings = param
        if 'train_data_' in parameters:
            self.train_data_ = self._arg(
                'train_data_',
                parameters.pop('train_data_'),
                DataFrame)
        if 'sort_data' in parameters:
            self.sort_data = self._arg('sort_data', parameters.pop('sort_data'), bool)

        if parameters:
            super(AutoTimeSeries, self).set_params(**parameters)
        return self

    def _check_params_before_fit(self, data, key, features): #pylint: disable=too-many-branches
        """
        Checks the validity of the parameters before fit().
        Infers certain parameters if not set (features, self.target and self.with_extra_predictable)

        Returns
        -------
        RuntimeError raised if something is wrong.
        Otherwise, features
        """
        if key:
            self.set_params(time_column_name=key)
        if not self.time_column_name:
            raise ValueError('The time column name is unknown. '
                             'Please set the time_column_name parameter.')
        if self.time_column_name not in data.columns:
            raise ValueError('The time column {} is missing in the dataset.'.format(
                self.time_column_name))

        if not self.target:
            cols = data.columns
            if self.time_column_name and (len(cols) == 2):
                # There are only 2 columns, we can deduce the target
                self.target = [col for col in cols if col != self.time_column_name][0]
        if not self.target:
            raise ValueError('The target column name is required.'
                             ' Please set the target parameter.')
        if self.target not in data.columns:
            raise ValueError('The target column {} is missing in the dataset.'.format(self.target))
        if not features:
            features = []
            if self.with_extra_predictable:
                # No features specified, take all as features except the target
                for col_name in data.columns:
                    if col_name != self.target:
                        features.append(col_name)
        if self.time_column_name not in features:
            features.append(self.time_column_name)
        if features == [self.time_column_name]:
            # If the features only contains DATE,
            # then reset 'with_extra_predictable' to false
            # (avoid APL from claiming about last_traning_time_point)
            self.with_extra_predictable = False
        return features

    def fit(self, data, key=None, features=None): #pylint: disable=too-many-branches
        """
        Fits the model.

        Parameters
        ----------
        data: hana_ml DataFrame
            The training dataset
        key: str, optional
            The column used as row identifier of the dataset.
            This column corresponds to the time column name.
            As a result, setting this parameter will overwrite the time_column_name model setting.
        features: list of str, optional
            The names of the feature columns, meaning the date column and the extra-predictive
            variables.
            If `features` is not provided, it defaults to all columns except the target column.

        Returns
        -------
        self: object
        """
        features = self._check_params_before_fit(data, key, features)
        self._set_conn_context(data.connection_context)
        new_view_name = None
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)
            self.train_data_ = data
            return super(AutoTimeSeries, self)._fit(
                data=data_sort,
                key=self.time_column_name,
                features=features,
                label=self.target)
        except dbapi.Error as db_er:
            logger.error("An issue occurred during model fitting: %s",
                         db_er, exc_info=True)
            self._drop_artifact_tables()
            raise
        finally:
            if new_view_name:
                self._try_drop_view([new_view_name])

    def predict(self, data, apply_horizon=None, apply_last_time_point=None):
        """
        Uses the fitted model to generate forecasts.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input dataset used for predictions
        apply_horizon: int, optional
            The number of forecasts to generate.
            By default, the number of forecasts is the horizon on which the model was trained.
        apply_last_time_point: str, optional
            The time point corresponding to the start of the forecast period. Forecasts will be
            generated starting from the next time point after the 'apply_last_time_point'.
            By default, this parameter is set to the value of 'last_training_time_point' known from
            the model training.

        Returns
        -------
        hana_ml DataFrame
            By default the output contains the following columns:
              - <the name of the time column>
              - ACTUAL: the actual value of time series
              - PREDICTED: the forecast value
              - LOWER_INT_95PCT: the lower limit of 95% confidence interval
              - UPPER_INT_95PCT: the upper limit of 95% confidence interval

            If ExtraMode is set to true, the output dataframe will also contain the breaking down
            of the time series into a trend, cycles, fluctuations and residuals components.

        Examples
        --------

        Default output

        >>> out = model.predict(hana_df)
        >>> out.collect().tail(5)
               Date            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.42329499999  6055.761105              NaN              NaN
        250  2001-12-28  7111.41669699999  6314.336098              NaN              NaN
        251  2002-01-03                           None  7033.88080      4529.46271      9538.29889
        252  2002-01-04                           None  6464.55722      3965.34339      8963.77104
        253  2002-01-07                           None  6469.14166      3961.41490      8976.86842


        Retrieving forecasts and components (predicted, trend, cycles and fluctuations). The output
        columns are suffixed with the horizon index. For example, Trend_1 means the trend component
        of the first horizon.

        >>> model.set_params(extra_applyout_settings={'ExtraMode': True})
        >>> out = model.predict(hana_df)
        >>> out.collect().tail(5)
                Date                               ACTUAL  PREDICTED_1      Trend_1  \
        249  2001-12-27  5995.423294999999598076101392507553  6055.761105  6814.405390   ...
        250  2001-12-28  7111.416696999999658146407455205917  6314.336098  6839.334762   ...
        251  2002-01-03                                 None  7033.880804  6991.163710   ...
        252  2002-01-04                                 None  6464.557223  7016.843985   ...
        253  2002-01-07                                 None  6469.141663  7094.528433   ...
        """
        if not apply_horizon:
            apply_horizon = self.horizon
        _, apply_config_data_df = self._create_operation_config_table(
            config_for_predict=True,
            apply_horizon=apply_horizon,
            apply_last_time_point=apply_last_time_point)
        new_view_name = None
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)

            applyout_df = super(AutoTimeSeries, self)._predict(
                data=data_sort,
                apply_config_data_df=apply_config_data_df
                )
            return self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("The model failed to generate forecasts: %s", db_er, exc_info=True)
            raise
        finally:
            if new_view_name:
                self._try_drop_view([new_view_name])

    def fit_predict(self, data, key=None, features=None, horizon=None):
        #pylint: disable=too-many-branches
        """
        Fits a model and generate forecasts in a single call to the FORECAST APL function.
        This method offers a faster way to perform the model training and forecasting.

        However, the user will not have access to the model used internally since it is deleted
        after the computation of the forecasts.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input time series dataset
        key: str, optional
            The date column name.
            By default, it is equal to the model parameter **time_column_name**.
            If it is given, the model parameter **time_column_name** will be overwritten.
        features: list of str, optional
            The column names corresponding to the extra-predictable variables (exogenous variables).
            If `features` is not provided, it is equal to all columns except the target column.
        horizon: int, optional
            The number of forecasts to generate.
            The default value equals to the horizon parameter of the model.

        Returns
        -------
        hana_ml DataFrame
            The output is the same as the predict() method.
        """
        if horizon:
            self.set_params(horizon=horizon)
        features = self._check_params_before_fit(data, key, features)
        self._set_conn_context(data.connection_context)
        new_view_name = None
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)

            applyout_df = self._call_apl_forecast(
                data=data_sort,
                key=self.time_column_name,
                features=features,
                target=self.target,
                horizon=horizon)
            self.train_data_ = data
            return self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("An issue was encounted during the model fitting: %s",
                         db_er, exc_info=True)
            raise
        finally:
            if new_view_name:
                self._try_drop_view([new_view_name])

    def forecast(self, forecast_length=None, data=None): #pylint: disable=too-many-branches
        """
        Uses the fitted model to generate out-of-sample forecasts.
        The model is supposed to be already fitted with a given dataset (training dataset).
        This method forecasts over a number of steps after the end of the training dataset.
        When there are extra-predictive variable (exogenous variables), the input parameter
        **data** is required. It must contain the values of the extra-predictable variables
        for the forecast period.
        If there is no extra-predictive variable, only the **forecast_length** parameter is needed.

        Parameters
        ----------
        forecast_length: int, optional
            The number of forecasts to generate from the end of the train dataset.
            This parameter is by default the horizon specified in the model parameter.
        data: hana_ml DataFrame, optional
            The time series with extra-predictable variables used for forecasting.
            This parameter is required if extra-predictive variables are used in the model.
            When this parameter is given, the parameter 'forecast_length' is ignored.

        Returns
        -------
        hana_ml DataFrame
            The output is the same as the predict() method.

        Examples
        --------

        Case where there is no extra-predictable variable:

        >>> train_df = DataFrame(CONN,
                                 'SELECT "Date" , "Cash" '
                                 'from APL_SAMPLES.CASHFLOWS_FULL '
                                 'where "Cash" is not null '
                                 'ORDER BY 1')
        >>> print(train_df.collect().tail(5))
                    Date         Cash
        246  2001-12-20  6382.441052
        247  2001-12-21  5652.882539
        248  2001-12-26  5081.372996
        249  2001-12-27  5995.423295
        250  2001-12-28  7111.416697

        >>> model = AutoTimeSeries(CONN, time_column_name='Date',
                                   target='Cash',
                                   horizon=3)
        >>> model.fit(train_df)
        >>> out = model.forecast(forecast_length=3)
        >>> out.collect().tail(5)
                   Date                        ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.42329499999901392507553  6814.405390              NaN              NaN
        250  2001-12-28  7111.41669699999907455205917  6839.334762              NaN              NaN
        251  2001-12-29                          None  6864.371407      -224.079492     13952.822306
        252  2001-12-30                          None  6889.515324      -211.264912     13990.295559
        253  2001-12-31                          None  6914.766513      -187.180923     14016.713949

        Case where there are extra-predictable variables:

        >>> train_df = DataFrame(CONN,
                                'SELECT * '
                                'from APL_SAMPLES.CASHFLOWS_FULL '
                                'WHERE "Cash" is not null '
                                'ORDER BY 1')
        >>> print(train_df.collect().tail(5))
                   Date  WorkingDaysIndices     ...       BeforeLastWMonth         Cash
        246  2001-12-20                  13     ...                      1  6382.441052
        247  2001-12-21                  14     ...                      1  5652.882539
        248  2001-12-26                  15     ...                      0  5081.372996
        249  2001-12-27                  16     ...                      0  5995.423295
        250  2001-12-28                  17     ...                      0  7111.416697

        >>> # Extra-predictable variables to be provided as the forecast period
        >>> forecast_df = DataFrame(CONN,
                                   'SELECT * '
                                   'from APL_SAMPLES.CASHFLOWS_FULL '
                                   'WHERE "Cash" is null '
                                   'ORDER BY 1 '
                                   'LIMIT 3')
        >>> print(forecast_df.collect())
                 Date  WorkingDaysIndices  ...   BeforeLastWMonth  Cash
        0  2002-01-03                   0  ...                  0  None
        1  2002-01-04                   1  ...                  0  None
        2  2002-01-07                   2  ...                  0  None

        >>> model = AutoTimeSeries(CONN,
                                   time_column_name='Date',
                                   target='Cash',
                                   horizon=3)
        >>> model.fit(train_df)
        >>> out = model.forecast(data=forecast_df)
        >>> out.collect().tail(5)
                   Date                          ACTUAL  PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.4232949999996101392507553    6814.41              NaN              NaN
        250  2001-12-28  7111.4166969999996407455205917    6839.33              NaN              NaN
        251  2001-12-29                            None    6864.37          -224.08         13952.82
        252  2001-12-30                            None    6889.52          -211.26         13990.30
        253  2001-12-31                            None    6914.77          -187.18         14016.71
        """

        if not self.model_ or not self.model_table_:
            raise FitIncompleteError('Please fit the model first.')
        if not self.train_data_:
            raise FitIncompleteError('The train dataset is unknown.'
                                     ' Please set the train_data_ parameter'
                                     ' or fit the model again.')
        cols = self.train_data_.columns
        if self.time_column_name and (len(cols) == 2):
            self.with_extra_predictable = False
        if not self.target:
            if not self.with_extra_predictable:
                # There are only 2 columns, we can deduce the target
                self.target = [col for col in cols if col != self.time_column_name][0]
            else:
                raise RuntimeError('Cannot create view. The target column is not defined.')
        if self.with_extra_predictable:
            if not data:
                raise RuntimeError("Please provide the extra-predictive values in the 'data'"
                                   " parameter")
        if data:
            forecast_length = data.count()
            # Checks if data has all required extra-predictable columns
            extra_prd_cols = data.columns
            for col in self.train_data_.columns:
                if col == self.target:
                    continue
                if col not in extra_prd_cols:
                    raise RuntimeError('Column %s is missing in the input dataset' % col)
        else:
            if not forecast_length:
                forecast_length = self.horizon
        if not forecast_length and not data:
            raise RuntimeError('Please set either forecast_length or data parameter')

        _, apply_config_data_df = self._create_operation_config_table(
            config_for_predict=True,
            apply_horizon=forecast_length,
            apply_last_time_point=self.last_training_time_point)
        new_view_name = None
        try:
            # Creates a view on the dataset extended with data
            # with ascending sort on the time column.
            new_view_name, data_sort = self._create_forecast_input_view(data)
            applyout_df = super(AutoTimeSeries, self)._predict(
                data=data_sort,
                apply_config_data_df=apply_config_data_df
                )
            return self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("Predict failed with error message: %s", db_er, exc_info=True)
            raise
        finally:
            if new_view_name:
                self._try_drop_view([new_view_name])

    def get_model_components(self):
        """
        Returns a dictionary containing the description of the model components, that is trend,
        cycles and fluctuations, used by the model to generate forecasts.

        Returns
        -------
        A dictionary with 3 possible keys: 'Trend', 'Cycles', 'Fluctuations'. For example:
            >>> model.get_model_components()
            {
                "Trend": "Linear(TIME)",
                 "Cycles": None,
                 "Fluctuations": "AR(36)"
            }
        """
        if not hasattr(self, 'indicators_'):
            raise FitIncompleteError(
                "The indicators table was not found. Please fit the model.")
        df_ind = self.indicators_
        cond = "KEY in ('Trend', 'Cycles', 'Fluctuations')"
        df_ind = df_ind.filter(cond)   # hana DataFrame
        df_ind = df_ind.collect()  # to pd.DataFrame
        ret = {row[1].KEY: row[1].VALUE for row in df_ind.iterrows()}
        return ret

    def get_performance_metrics(self):
        """
        Returns a dictionary containing the performance metrics of the model.
        The metrics are provided for each forecast horizon.

        Returns
        -------
        Dictionary
            The dictionary contains the performance metrics of the current model.
            Each metric is associated to a list containing <horizon> elements.
            This list contains the values of the metric measured for horizon 1 to <horizon>.

        Example
        -------

        A model is trained with 4 horizons. The returned value will be:
        ::
        >>> model.get_performance_metrics()
        {'MAPE': [
              0.1529961017445385,
              0.1538823292343699,
              0.1564376267423695,
              0.15170398377407046}
        """
        if not hasattr(self, 'indicators_'):
            raise FitIncompleteError(
                "The indicators table was not found. Please fit the model.")
        df_ind = self.indicators_
        # Get metrics per horizon (MAPE)
        cond = "VARIABLE is null and TARGET is null and DETAIL is not null"
        df_ind = df_ind.filter(cond)   # hana_ml DataFrame
        df_ind = df_ind.collect()  # pd.DataFrame
        # Extracts the horizon number from the text 'Forecast n'
        df_ind['HORIZON_NO'] = df_ind['DETAIL'].str.extract(r'Forecast (\d+)')
        df_ind['HORIZON_NO'] = df_ind['HORIZON_NO'].astype(int)
        df_ind.sort_values(by=['KEY', 'HORIZON_NO'], inplace=True)
        df_ind['VALUE'] = df_ind['VALUE'].astype(float)
        df_metrics = df_ind[['KEY', 'HORIZON_NO', 'VALUE']]

        # Pivots table so we will have:
        # HORIZON_NO   1    2    3    4    5    6    7    8    9     10
        # KEY
        # MAPE        1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  10.0
        df_metrics = df_metrics.pivot(index='KEY', columns='HORIZON_NO', values='VALUE')

        # Converts to dictionary for return
        ret = {metric_name: [val for _, val in vals.iteritems()]
               for metric_name, vals in df_metrics.iterrows()}
        return ret

    def get_horizon_wide_metric(self, metric_name='MAPE'):
        """
        Returns value of performance metric (MAPE, sMAPE, ...) averaged on the forecast horizon.

        Parameters
        ----------
        metric_name: str
            Default value equals 'MAPE'.
            Possible values: 'MAPE', 'MPE', 'MeanAbsoluteError', 'RootMeanSquareError', 'SMAPE',
            'L1', 'L2', 'P2', 'R2', 'U2'

        Returns
        -------
        float
            The metric value averaged on the forecast horizon.
            It is based on validation partition.
        """
        metrics = self.get_performance_metrics()
        if metrics:
            return np.mean(metrics[metric_name])
        raise FitIncompleteError("No metrics available. Please fit the model first.")

    def load_model(self, schema_name, table_name, oid=None):
        """
        Loads the model from a table.

        Parameters
        ----------
        schema_name: str
            The schema name
        table_name: str
            The table name
        oid : str, optional
            If the table contains several models,
            the OID must be given as an identifier.
            If it is not provided, the whole table is read.

        Note
        ----

        Before using a reloaded model for a new prediction, set the following parameters again:
        'time_column_name', 'target' and 'train_data_'.
        The SAP HANA ML library needs these parameters to prepare the dataset view.
        Otherwise, methods such as forecast() and predict() will fail.

        Example
        -------

        >>> # Sets time_column_name and target again
        >>> model = AutoTimeSeries(conn_context=CONN, time_column_name='Date', target='Cash')
        >>> model.load_model(schema_name='MY_SCHEMA',
        >>>          table_name='MY_MODEL_TABLE',
        >>>          )
        >>> model.predict(hana_df,
                        apply_horizon=(NB_HORIZON_TRAIN + 5),
                        apply_last_time_point=LAST_TRAIN_DATE)

        """
        super(AutoTimeSeries, self).load_model(schema_name=schema_name,
                                               table_name=table_name,
                                               oid=oid)
        if not self.time_column_name:
            logger.warning("The time_column_name parameter is empty."
                           "Please set it to a correct value before making predictions.")
        if not self.target:
            logger.warning("The target parameter is empty."
                           "Please set it to a correct value before making predictions.")
        if not self.train_data_:
            logger.warning("The train_data_ parameter is empty."
                           "Please set it to a correct value before making predictions.")

    def _create_train_config_table(self):
        """
        Creates a new APLArtifactTable object for the "TRAIN_OPERATION_LOG" table.

        Returns:
        ------
        An APLArtifactTable object with data
        """
        config_table, _ = self._create_operation_config_table(config_for_predict=False)
        return config_table

    def _create_forecast_config_table(self, horizon=None):
        """
        Creates an "OPERATION_LOG" table specifically for the FORECAST function.

        Returns:
        ------
        An APLArtifactTable object with data
        """
        config_table, _ = self._create_operation_config_table(
            config_for_predict=True, apply_horizon=horizon)
        return config_table

    def _create_operation_config_table(self,
                                       config_for_predict=False,
                                       apply_horizon=None,
                                       apply_last_time_point=None):
        #pylint: disable=too-many-branches
        """
        Creates an "OPERATION_CONFIG" artifact table for APPLY_MODEL.

        Parameters
        ----------
        config_for_predict: bool,
            True if the model is being applied.
            False if the model is being trained.
        apply_horizon: int,
            Number of steps to be forecasted
        apply_last_time_point: str or date
            The date of the time point from which the actual will not be used in forecasting

        Returns
        -------
        (APLArtifact, pd.DataFrame)
        They all point to the "OPERATION_CONFIG" table, for predict() or forecast().
        """
        if self._model_type is None:
            raise FitIncompleteError("Model type undefined.")
        config_ar = [('APL/ModelType', self._model_type, None)]
        # Adds params to OPERATION_CONFIG table
        if self.time_column_name:
            config_ar.append(('APL/TimePointColumnName', self.time_column_name, None))

        if apply_horizon:
            config_ar.append(('APL/AppliedHorizon', str(apply_horizon), None))
        if self.horizon:
            config_ar.append(('APL/Horizon', str(self.horizon), None))

        if self.with_extra_predictable:
            config_ar.append(('APL/WithExtraPredictable', 'true', None))
        else:
            config_ar.append(('APL/WithExtraPredictable', 'false', None))
        if self.last_training_time_point:
            config_ar.append(('APL/LastTrainingTimePoint',
                              self.last_training_time_point, None))
        else:
            # Enables autoDetectLastRow
            param = 'Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/AutoDetectLastRow'
            config_ar.append((param, 'true', None))
        # Sets ExtraMode for predictions output
        if config_for_predict:
            use_default_cfg = True
            if hasattr(self, 'extra_applyout_settings') and self.extra_applyout_settings:
                # if extra_applyout_settings is given
                extra_mode = self.extra_applyout_settings['ExtraMode']
                extra_mode = bool(extra_mode)
                if extra_mode:
                    config_ar.append(('APL/ApplyExtraMode',
                                      'Stable Components and Error Bars',
                                      None))
                    use_default_cfg = False
            if use_default_cfg:
                # Default output
                config_ar.append(('APL/ApplyExtraMode', 'Forecasts and Error Bars', None))

        if apply_last_time_point:
            config_ar.append(('APL/ApplyLastTimePoint', apply_last_time_point, None))

        # Adds other params to the "OPERATION_CONFIG" table
        config_ar = config_ar + self._get_train_config_data()
        config_df = pd.DataFrame(config_ar)
        if config_df is None:
            raise FitIncompleteError("Train configuration undefined.")
        config_table = self._create_aplartifact_table_with_data_frame(
            name='#CREATE_AND_TRAIN_CONFIG_{}'.format(self.id),
            type_name=APLArtifactTable.OPERATION_CONFIG_EXTENDED,
            data_df=config_df
            )
        return config_table, config_df

    def _rewrite_applyout_df(self, applyout_df):
        #pylint: disable=too-many-branches
        #pylint: disable=too-many-statements
        #pylint: disable=too-many-locals
        """
        Rewrites the applyout dataframe so it outputs standardized column names.
        Parameters:
        ---------
        applyout_df : hana_ml DataFrame
            The initial output of predict

        Returns
        ------
        A new hana_ml DataFrame with renamed columns
        """

        # Determines the mapping of old columns to new columns
        # Stores the mapping into different list of tuples [(old_column, new_columns)]
        start_cols = []   # starting columns: ID, ACTUAL
        predicted_cols = []  # [('kts_1', 'PREDICTED_1), ('kts_2', 'PREDICTED_2), ...]
        components_cols = []  # Same thing for components [('kts_1AR', 'AR_1), ...]
        residues_cols = []  # for residues
        misc_cols = []   # others
        for old_col in applyout_df.columns:
            # if old_col in [self.time_column_name, self.target]:
            if old_col == self.time_column_name:
                new_col = old_col
                start_cols.append((old_col, new_col))
            elif old_col == self.target:
                new_col = 'ACTUAL'
                start_cols.append((old_col, new_col))
            elif (old_col == 'kts_1') and (self.extra_applyout_settings is None):
                new_col = 'PREDICTED'
                start_cols.append((old_col, new_col))
            elif old_col == 'kts_1_lowerlimit_95%':
                new_col = 'LOWER_INT_95PCT'
                start_cols.append((old_col, new_col))
            elif old_col == 'kts_1_upperlimit_95%':
                new_col = 'UPPER_INT_95PCT'
                start_cols.append((old_col, new_col))
            elif self.extra_applyout_settings:
                # ExtraMode is set.
                # many columns are now available: kts_n, kts_n<component>, kts_nResidues<component>
                # kts_[n}Residues{Component} -> {Component}_RESIDUALS_{n}
                found = self._get_new_column_name(
                    old_col_re=r'kts_([0-9]+)Residues([a-zA-Z_]+)',
                    old_col=old_col,
                    new_col_re=r'\2_RESIDUALS_\1')
                if found:
                    new_col = found
                    residues_cols.append((old_col, new_col))
                else:
                    # kts_[n}{Component} -> {Component}_{n}
                    found = self._get_new_column_name(
                        old_col_re=r'kts_([0-9]+)([a-zA-Z_]+)',
                        old_col=old_col,
                        new_col_re=r'\2_\1')
                    if found:
                        new_col = found
                        components_cols.append((old_col, new_col))
                    else:
                        found = self._get_new_column_name(
                            old_col_re=r'kts_([0-9]+)',
                            old_col=old_col,
                            new_col_re=r'PREDICTED_\1')
                        if found:
                            new_col = found
                            predicted_cols.append((old_col, new_col))
            else:
                new_col = old_col
                misc_cols.append((old_col, new_col))
        # Writes the select SQL by renaming the columns
        sql = 'SELECT '
        # Starting columns
        for i, (old_col, new_col) in enumerate(start_cols):
            if i > 0:
                sql = sql + ', '
            sql = (sql + '{old_col} {new_col}'.format(
                old_col=quotename(old_col),
                new_col=quotename(new_col)))

        # kts_*, kts_*Components, kts_*Residues columns
        # Ordered by horizon, predicted, components, residues
        nb_horizons = len(predicted_cols)
        nb_components = 0
        if nb_horizons > 0:
            nb_components = int(len(components_cols)/nb_horizons)
        for horizon in range(nb_horizons):
            old_col, new_col = predicted_cols[horizon]
            sql = sql + ', '
            sql = (sql + '{old_col} {new_col}'.format(
                old_col=quotename(old_col),
                new_col=quotename(new_col)))
            for l_maps in [components_cols, residues_cols]:
                if l_maps:  # if the mapping is not empty
                    for i_component in range(nb_components):
                        old_col, new_col = l_maps[(horizon * nb_components) + i_component]
                        sql = sql + ', '
                        sql = (sql + '{old_col} {new_col}'.format(
                            old_col=quotename(old_col),
                            new_col=quotename(new_col)))
        sql = (sql
               + ' FROM ' + self.applyout_table_.name
               + ' ORDER BY ' + quotename(self.time_column_name))
        applyout_df_new = DataFrame(connection_context=self.conn_context,
                                    select_statement=sql)
        logger.info('DataFrame for predict ouput: %s', sql)
        return applyout_df_new

    def _create_sorted_view(self, data):
        """
        Creates a view on the dataset with ascending sort on the time column.
        It is required for APL TimeSeries.
        However, if the parameter 'sort_data' is False, the view is not created.
        Parameters
        ----------
        data : hana_ml DataFrame
            Input dataset

        Returns
        -------
        (new_view_name, data_sort)
            new_view_name : the newly created view name or None if no view is created
            data_sort : the new hana_ml DataFrame mapped to this new view or the original dataframe

        Note
        ----
        The dataset must not be a temporary table, otherwise the creation will fail in SAP HANA.
        """
        if not self.sort_data:
            # User can provide directly a (tmp) sorted view. View must not be created.
            return None, data
        if not self.time_column_name:
            raise Error('Cannot create view. time_column_name parameter is not defined.')
        new_view_name = 'TS_DATA_SORT_VIEW_{}'.format(self.id)
        self._create_view(view_name=new_view_name,
                          data=data,
                          order_by=quotename(self.time_column_name))
        data_sort = DataFrame(self.conn_context, 'select * from ' + new_view_name)
        return new_view_name, data_sort

    def _create_forecast_input_view(self, extra_pred_data):
        """
        Creates a view on the dataset extended with the out-of-sample extra-predictable variables.
        The resulted view will be used by the forecast() method.

        Returns
        ------
        (new_view_name, data_sort)
            new_view_name : the newly created view
            data_sort : the new hana_ml DataFrame mapped to this new view
        """
        if extra_pred_data:
            # Union train_dataset + extra_pred_data
            sql = 'select * from ({TRAIN}) union '
            if self.target in extra_pred_data.columns:
                sql = (sql +
                       'select * from ({EXTRA_PRED})')
            else:
                sql = (sql +
                       'select *, null {TARGET_VAR} from ({EXTRA_PRED})')
            sql = sql.format(TRAIN=(self.train_data_.select_statement),
                             TARGET_VAR=quotename(self.target),
                             EXTRA_PRED=extra_pred_data.select_statement,
                            )
            new_df = DataFrame(connection_context=self.conn_context, select_statement=sql)
        else:
            # When there is no extra-predictable variable, forecast is only based on train dataset
            # + forecast_length
            new_df = self.train_data_
        return self._create_sorted_view(new_df)

    def _create_forecast_out(self, data, horizon):
        #pylint: disable=too-many-locals
        """
        Creates the Table object for the FORECAST APL function output (fit_predict() method).
        Because we are limited to a single call to FORECAST function, the table definition cannot
        be determined via intermediary calls to APL.
        Hence, we have to hard-code the table definition based on the extra_applyout_settings.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input dataset
        horizon: int
            The horizon
        Returns
        -------
        A APLArtifactApplyOutTable object containing the table definition
        """

        if not horizon:
            horizon = self.horizon

        data_types_dict = {}  # {'ColumnName' : 'HanaType'}
        for dtype in data.dtypes():
            name, sqltype = dtype[0], dtype[1]
            if sqltype == 'DECIMAL':
                # parse_one_dtype raise TypeError if DECIMAL
                colname, sqltype = (name, 'DOUBLE')
            else:
                colname, sqltype = parse_one_dtype(dtype)
            data_types_dict[colname] = sqltype

        use_default_cfg = True
        if hasattr(self, 'extra_applyout_settings') and self.extra_applyout_settings:
            extra_mode = self.extra_applyout_settings['ExtraMode']
            extra_mode = bool(extra_mode)
            if extra_mode:
                applyout_tbl_specs = [
                    (self.time_column_name, data_types_dict[self.time_column_name]),
                    (self.target, data_types_dict[self.target]),
                    ]
                for i in range(1, horizon + 1):
                    applyout_tbl_specs.append(('kts_%d' % i, 'DOUBLE'))
                applyout_tbl_specs.extend([('kts_1_lowerlimit_95%', 'DOUBLE'),
                                           ('kts_1_upperlimit_95%', 'DOUBLE')])
                for hor in range(1, horizon + 1):
                    for comp in ['Trend', 'Cycles', 'Fluctuations']:
                        # Trend, Cycles, Fluctuations
                        applyout_tbl_specs.extend([('kts_{}{}'.format(hor, comp), 'DOUBLE'),
                                                   ('kts_{}Residues{}'.format(hor, comp), 'DOUBLE')
                                                  ])
                use_default_cfg = False
        if use_default_cfg:
            # default output
            applyout_tbl_specs = [
                (self.time_column_name, data_types_dict[self.time_column_name]),
                (self.target, data_types_dict[self.target]),
                ('kts_1', 'DOUBLE'),
                ('kts_1_lowerlimit_95%', 'DOUBLE'),
                ('kts_1_upperlimit_95%', 'DOUBLE'),
                ]

        # Create Apply-out table
        applyout_tbl_name = '#APPLYOUT_%s' % (self.id)  # name of table
        # applyout_table = Table(applyout_tbl_name, applyout_tbl_specs)
        applyout_table_def = ', '.join(['{col_name} {col_type}'.format(col_name=quotename(col_name),
                                                                       col_type=col_type)
                                        for col_name, col_type in applyout_tbl_specs
                                       ]
                                      )
        applyout_table_def = '({})'.format(applyout_table_def)
        applyout_table = APLArtifactApplyOutTable(
            name=applyout_tbl_name,
            table_definition=applyout_table_def,
            apl_version=self._apl_version,
            data=[]
        )
        return applyout_table

    def _call_apl_forecast(self, data, key, features, target, horizon):
        #pylint: disable=too-many-arguments
        #pylint: disable=too-many-branches
        #pylint:disable=too-many-locals
        #pylint: disable=too-many-statements
        """
        Calls the FORECAST APL function.

        Parameters
        ----------
        data : DataFrame
            The training dataset
        key : str, optional
            The name of the ID column
        features : list of str, optional
            The names of the feature columns
        target : str, optional
            The name of the target variable
        horizon : int, optional
            The number of forecasts

        """
        # -- get and check the validity of input params key, features, target
        key = self._arg('key', key, str)
        features = self._arg('features', features, ListOfStrings)
        target = self._arg('target', target, str)
        self._check_valid(data, key, features, target)

        # -- prepare TRAIN_CONFIG artifact
        forecast_config_table = self._create_forecast_config_table(horizon)

        try:
            # ---- prepare train dataset
            # Before doing a guess variable, need to materialize the dataset.
            # We can either make a view or create a new table
            # View offers more performance
            # But view is impossible when the dataset is temporary table
            try:
                data_view_name = "TRAIN_DATA_VIEW_{}".format(self.id)
                self._create_view(view_name=data_view_name, data=data)
            except dbapi.Error:
                # fall back if view can't be created
                # (because the original dataset is a temp table)
                data_view_name = "#TRAIN_DATA_VIEW_{}".format(self.id)
                self._materialize_w_type_conv(name=data_view_name, data=data)

            # applyout table
            applyout_table = self._create_forecast_out(data, horizon)

            # ---- VARIABLE_DESCRIPTION
            var_desc_table = self._create_var_desc_table(key, target, data_view_name)

            # --- HEADER_FUNCTION
            func_header_table = self._create_func_header_table()

            # --- VARIABLE_ROLES
            var_roles_table = self._create_var_roles_table(
                data=data,
                key=key,
                label=target,
                features=features,
                weight=None,
            )

            # --- Prepare Output tables
            # OPERATION_LOG,
            operation_log_table = self._create_operation_log_table(
                '#FIT_LOG_{}'.format(self.id))

            # SUMMARY,
            summary_table = APLArtifactTable(
                name='#SUMMARY_{}'.format(self.id),
                type_name=APLArtifactTable.SUMMARY,
                apl_version=self._apl_version)

            # INDICATORS
            indicators_table = APLArtifactTable(
                name='#INDICATORS_{}'.format(self.id),
                type_name=APLArtifactTable.INDICATORS,
                apl_version=self._apl_version)

            # Materialize artifacts prior to calling APL
            self._create_artifact_table(func_header_table)
            self._create_artifact_table(forecast_config_table)
            self._create_artifact_table(var_desc_table)
            self._create_artifact_table(var_roles_table)
            self._create_artifact_table(applyout_table)
            #self._artifact_tables[applyout_table.name] = applyout_table
            self._create_artifact_table(operation_log_table)
            self._create_artifact_table(summary_table)
            self._create_artifact_table(indicators_table)

            # --- Call procedure
            # If DU is used, applyout_table must be passed as string
            applyout_table_arg = applyout_table if self._with_overview_option != 2 \
                else applyout_table.name
            self._call_apl(
                "APL_FORECAST",
                input_tables=[
                    func_header_table,
                    forecast_config_table,
                    var_desc_table,
                    var_roles_table,
                    data_view_name,
                    applyout_table_arg,
                ],
                output_tables=[
                    operation_log_table,
                    summary_table,
                    indicators_table
                ])
        except dbapi.Error as db_er:
            # do stuff with the error, and also raise to the caller,
            # clean up the table used in fit function
            logger.error("An issue was encounted during the model fitting: %s",
                         db_er, exc_info=True)
            self._drop_artifact_tables()
            raise
        finally:
            # clean created view
            self._try_drop_view(data_view_name)

        # --- Save returned artifacts
        # No model returned by APL_FORECAST
        self.model_table_ = None
        self.model_ = None
        # It is useless to keep the model as Dataframe, just for compatibility
        # --- capture the other output artifacts as hana Dataframes
        self.summary_ = self.conn_context.table(summary_table.name) #pylint:disable=attribute-defined-outside-init
        self.indicators_ = self.conn_context.table(indicators_table.name) #pylint:disable=attribute-defined-outside-init
        self.fit_operation_log_ = self.conn_context.table( #pylint:disable=attribute-defined-outside-init
            operation_log_table.name)
        self.predict_operation_log_ = self.fit_operation_log_  #pylint:disable=attribute-defined-outside-init
        self.var_desc_ = self.conn_context.table(var_desc_table.name) #pylint:disable=attribute-defined-outside-init

        self.applyout_ = self.conn_context.table(applyout_table.name) #pylint:disable=attribute-defined-outside-init
        self.applyout_table_ = applyout_table #pylint:disable=attribute-defined-outside-init
        return self.applyout_
